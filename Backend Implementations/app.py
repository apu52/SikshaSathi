from flask import Flask, request, jsonify
from flask_cors import CORS
from sentence_transformers import SentenceTransformer, util
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
import yake
import spacy
from gensim import corpora
from gensim.models import LdaModel
from nltk.corpus import stopwords
import PyPDF2
import matplotlib.pyplot as plt

nltk.download('punkt')
nltk.download('stopwords')

# Initialize Flask app
app = Flask(__name__)
CORS(app)

# Load NLP models
nlp = spacy.load("en_core_web_sm")
stop_words = set(stopwords.words('english'))
bert_model = SentenceTransformer("all-MiniLM-L6-v2")
tfidf_vectorizer = TfidfVectorizer()

# Keyword Extraction
def extract_keywords(text, num_keywords=5):
    kw_extractor = yake.KeywordExtractor(top=num_keywords, stopwords=None)
    keywords = [kw[0].lower() for kw in kw_extractor.extract_keywords(text)]
    return keywords

# Compute TF-IDF Similarity
def compute_tfidf_similarity(teacher_text, student_text):
    if not student_text.strip():
        return 0.0
    tfidf_vectorizer.fit([teacher_text])  # Fit only on the teacher text
    tfidf_matrix = tfidf_vectorizer.transform([teacher_text, student_text])
    similarity = (tfidf_matrix * tfidf_matrix.T).toarray()[0, 1] * 100
    return round(similarity, 2)

# Compute BERT Similarity
def compute_bert_similarity(teacher_text, student_text):
    if not student_text.strip():
        return 0.0
    sim_score = util.pytorch_cos_sim(bert_model.encode(teacher_text), bert_model.encode(student_text))
    return round(sim_score.item() * 100, 2)

# Compute Topic Similarity
def compute_topic_similarity(master_text, student_text, num_topics=3):
    tokenized_texts = [[word for word in nltk.word_tokenize(text.lower()) if word not in stop_words] 
                       for text in [master_text, student_text]]
    dictionary = corpora.Dictionary(tokenized_texts)
    corpus = [dictionary.doc2bow(text) for text in tokenized_texts]
    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)
    master_topics = dict(lda_model[corpus[0]])
    student_topics = dict(lda_model[corpus[1]])
    overlap = sum(min(master_topics.get(topic, 0), student_topics.get(topic, 0)) for topic in master_topics)
    return round(overlap * 100, 2)

# AI Plagiarism Check
def check_ai_plagiarism(student_text, threshold=90):
    ai_generated_sample = "This is a perfectly structured response generated by an AI model."
    student_embedding = bert_model.encode(student_text)
    ai_embedding = bert_model.encode(ai_generated_sample)
    similarity = util.pytorch_cos_sim(student_embedding, ai_embedding).item() * 100
    return similarity > threshold, round(similarity, 2)

# Student Plagiarism Check
def check_student_plagiarism(student_texts, threshold=80):
    embeddings = [bert_model.encode(text) for text in student_texts]
    plagiarism_flags = {}
    for i, emb1 in enumerate(embeddings):
        for j, emb2 in enumerate(embeddings[i+1:], start=i+1):
            sim = util.pytorch_cos_sim(emb1, emb2).item() * 100
            if sim > threshold:
                plagiarism_flags[f"Student {i} vs Student {j}"] = round(sim, 2)
    return plagiarism_flags

# Generate Improvement Suggestions
def generate_suggestions(section_name, section_text, master_keywords, bert_similarity, threshold=60):
    suggestions = []
    if bert_similarity < threshold:
        missing_keywords = [kw for kw in master_keywords if kw not in section_text.lower()]
        if missing_keywords:
            suggestions.append(f"{section_name}: Add {', '.join(missing_keywords)}.")
        else:
            suggestions.append(f"{section_name}: Expand content to match master copy.")
    return suggestions

# API Endpoint: Evaluate Student Response
@app.route("/evaluate", methods=["POST"])
def evaluate():
    try:
        data = request.get_json()
        if not data or "master_copy" not in data or "student_texts" not in data:
            return jsonify({"error": "Invalid input. Provide master_copy and student_texts."}), 400

        master_copy = data["master_copy"]
        student_texts = data["student_texts"]
        
        # Run plagiarism check once for all students
        all_plagiarism_flags = check_student_plagiarism(student_texts)
        
        results = []
        for i, student_text in enumerate(student_texts, 1):
            tfidf_similarity = compute_tfidf_similarity(master_copy, student_text)
            bert_similarity = compute_bert_similarity(master_copy, student_text)
            topic_similarity = compute_topic_similarity(master_copy, student_text)
            is_ai_plagiarized, ai_similarity = check_ai_plagiarism(student_text)
            master_keywords = extract_keywords(master_copy)
            suggestions = generate_suggestions(f"Student {i}", student_text, master_keywords, bert_similarity)
            
            results.append({
                "Student": i,
                "TF-IDF Similarity": tfidf_similarity,
                "BERT Similarity": bert_similarity,
                "Topic Similarity": topic_similarity,
                "AI Plagiarism": {"Detected": is_ai_plagiarized, "Similarity": ai_similarity},
                "Plagiarism Flags": all_plagiarism_flags.get(f"Student {i}", {}),
                "Improvement Suggestions": suggestions
            })
        
        return jsonify({"Results": results})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Run Flask App
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)